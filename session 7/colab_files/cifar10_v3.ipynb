{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imb7syhCpIZM",
        "outputId": "b40e3079-1645-4b76-afa0-0c9790414abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Use Group or Depthwise Convolutions**\n",
        "\n",
        "  This strategy drastically reduces parameters by splitting convolutions across channel groups instead of applying full convolutions over all channels.\n",
        "  It’s a core idea used in MobileNet and other efficient CNN architectures.\n",
        "\n",
        "What changes\n",
        "\n",
        "In your model, the ConvBlock4 layer is the biggest contributor to parameters.\n",
        "We’ll replace it with a depthwise separable convolution:\n",
        "\n",
        " * Depthwise convolution — applies a separate kernel per input channel (parameter count much smaller).\n",
        "\n",
        " * Pointwise convolution — 1×1 convolution to combine features (cheap relative to full conv).\n",
        "\n",
        "\n",
        "This reduces parameters massively while preserving receptive field and performance."
      ],
      "metadata": {
        "id": "PnEedts4eGjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **CNN Parameter Reduction Strategies — Cheat Sheet**\n",
        "\n",
        "#### **1. Reduce Output Channels**\n",
        "\n",
        "* **How:** Lower the number of output channels in heavy convolution layers.\n",
        "* **When:** When a layer has disproportionately large parameters (usually deeper layers).\n",
        "* **Formula:**\n",
        "  [\n",
        "  \\text{Params} = C_{\\text{in}} × C_{\\text{out}} × K × K\n",
        "  ]\n",
        "* **Example:** Changing Conv2d(128 → 215, 3×3) → Conv2d(128 → 144, 3×3) drastically reduces parameters.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 1×1 Convolution (Bottleneck Layer)**\n",
        "\n",
        "* **How:** Insert a 1×1 convolution before expensive convolution layers to reduce channel depth.\n",
        "* **When:** Before layers where (C_{\\text{in}}) is high, to compress features before heavy computation.\n",
        "* **Formula:**\n",
        "  [\n",
        "  \\text{Params}*{\\text{bottleneck}} = C*{\\text{in}} × C_{\\text{reduced}} × 1 × 1\n",
        "  ]\n",
        "  [\n",
        "  \\text{Params}*{\\text{conv}} = C*{\\text{reduced}} × C_{\\text{out}} × K × K\n",
        "  ]\n",
        "* **Example:**\n",
        "  128 → 96 channels bottleneck before ConvBlock4 can cut parameters significantly.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Depthwise Separable Convolution**\n",
        "\n",
        "* **How:** Replace a normal convolution with depthwise convolution followed by a pointwise convolution.\n",
        "* **When:** When aiming for large parameter savings without losing much performance (mobile-friendly design).\n",
        "* **Formula:**\n",
        "  [\n",
        "  \\text{Params} = C_{\\text{in}} × K × K + C_{\\text{in}} × C_{\\text{out}} × 1 × 1\n",
        "  ]\n",
        "* **Example:**\n",
        "  ConvBlock4: 128 → 215 channels (3×3)\n",
        "  Normal: (128×215×3×3 = 247,680)\n",
        "  Depthwise separable: (128×3×3 + 128×215×1×1 = 28,672) (~8× reduction).\n",
        "\n",
        "---\n",
        "\n",
        "#### **General Variables**\n",
        "\n",
        "* (C_in) = Input channels\n",
        "* (C_out) = Output channels\n",
        "* (K) = Kernel size\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4FTb03p0fFLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ciferNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ciferNet, self).__init__()\n",
        "\n",
        "        # Conv Block 1\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "\n",
        "        # Conv Block 2\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=2, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "\n",
        "        # Conv Block 3\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        # Conv Block 4 → Depthwise separable convolution\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1, groups=128, bias=False),  # Depthwise conv\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 1), bias=False),  # Pointwise conv\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "\n",
        "        # GAP and classifier\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Conv2d(in_channels=256, out_channels=10, kernel_size=(1, 1), bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return F.log_softmax(x, dim=-1)\n"
      ],
      "metadata": {
        "id": "Sz69nwr8tH1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = ciferNet().to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl2VtBeALu2W",
        "outputId": "fe170e46-7861-4a4f-e402-f0190b0223ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "              ReLU-2           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-3           [-1, 16, 32, 32]              32\n",
            "            Conv2d-4           [-1, 32, 16, 16]           4,608\n",
            "              ReLU-5           [-1, 32, 16, 16]               0\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "            Conv2d-7           [-1, 64, 16, 16]          18,432\n",
            "              ReLU-8           [-1, 64, 16, 16]               0\n",
            "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
            "           Conv2d-10             [-1, 64, 8, 8]          36,864\n",
            "             ReLU-11             [-1, 64, 8, 8]               0\n",
            "      BatchNorm2d-12             [-1, 64, 8, 8]             128\n",
            "           Conv2d-13            [-1, 128, 8, 8]          73,728\n",
            "             ReLU-14            [-1, 128, 8, 8]               0\n",
            "      BatchNorm2d-15            [-1, 128, 8, 8]             256\n",
            "           Conv2d-16            [-1, 128, 8, 8]           1,152\n",
            "             ReLU-17            [-1, 128, 8, 8]               0\n",
            "      BatchNorm2d-18            [-1, 128, 8, 8]             256\n",
            "           Conv2d-19            [-1, 256, 8, 8]          32,768\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-21            [-1, 256, 8, 8]             512\n",
            "AdaptiveAvgPool2d-22            [-1, 256, 1, 1]               0\n",
            "           Conv2d-23             [-1, 10, 1, 1]           2,560\n",
            "================================================================\n",
            "Total params: 171,920\n",
            "Trainable params: 171,920\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.78\n",
            "Params size (MB): 0.66\n",
            "Estimated Total Size (MB): 2.45\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.4741, 0.4727, 0.4733),(0.2521, 0.2520, 0.2506),)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.4741, 0.4727, 0.4733),(0.2521, 0.2520, 0.2506),)\n",
        "                                       ])"
      ],
      "metadata": {
        "id": "Xo-t0YLCMA9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.CIFAR10('./data', train=False, download=True, transform=test_transforms)"
      ],
      "metadata": {
        "id": "YhgskLdmOZFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "train_dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "test_dataloader_args = dict(shuffle=False, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=False, batch_size=64)\n",
        "\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **train_dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **test_dataloader_args)"
      ],
      "metadata": {
        "id": "YqXdGOhTOyrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80b2128-dbab-4ffd-bb59-c6e7e1414d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss.item()) # Added .item()\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.float().argmax(dim=1, keepdim=True)  # get the index of the max log-probability, convert to float\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.float().argmax(dim=1, keepdim=True)  # get the index of the max log-probability, convert to float\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss) # Added .item()\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "LT_W_lCuPb_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  ciferNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 8\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    # scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "iEB6WDqsPdHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90797947-4bb8-4ba2-c139-f49a3cae20bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.31123948097229 Batch_id=390 Accuracy=46.62: 100%|██████████| 391/391 [00:19<00:00, 20.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2479, Accuracy: 5539/10000 (55.39%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.0990206003189087 Batch_id=390 Accuracy=62.07: 100%|██████████| 391/391 [00:14<00:00, 27.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0047, Accuracy: 6349/10000 (63.49%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9283403158187866 Batch_id=390 Accuracy=69.55: 100%|██████████| 391/391 [00:13<00:00, 28.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8288, Accuracy: 7072/10000 (70.72%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6603173017501831 Batch_id=390 Accuracy=74.61: 100%|██████████| 391/391 [00:14<00:00, 27.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7589, Accuracy: 7364/10000 (73.64%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6709796786308289 Batch_id=390 Accuracy=78.39: 100%|██████████| 391/391 [00:14<00:00, 27.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7179, Accuracy: 7495/10000 (74.95%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5217894315719604 Batch_id=390 Accuracy=81.28: 100%|██████████| 391/391 [00:13<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7135, Accuracy: 7507/10000 (75.07%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.4977995455265045 Batch_id=390 Accuracy=83.12: 100%|██████████| 391/391 [00:14<00:00, 27.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6444, Accuracy: 7776/10000 (77.76%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.4013732969760895 Batch_id=390 Accuracy=85.24: 100%|██████████| 391/391 [00:13<00:00, 28.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6742, Accuracy: 7720/10000 (77.20%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbK60O1UPfvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}