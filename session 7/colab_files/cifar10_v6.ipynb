{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imb7syhCpIZM",
        "outputId": "5fc57a59-eb5e-4303-d000-02f30aeb853e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's adjust the **dilation factor**, **kernel sizes**, and **stride** to bring the receptive field down closer to your target range.\n",
        "\n",
        "### Adjustments:\n",
        "\n",
        "* **Dilation**: We can reduce the dilation factor to **2** to shrink the receptive field while still using dilation to maintain the global context.\n",
        "* **Kernel Size**: We’ll use smaller kernel sizes than (7 \\times 7) in some layers to reduce the receptive field growth rate.\n",
        "* **Stride**: We’ll make sure that the strides don’t downsample the feature maps too aggressively, keeping more spatial information at later stages.\n",
        "\n",
        "Let’s aim to use a **dilation of 2** (instead of 3) and use **(5 \\times 5) kernels** for some layers, instead of the larger (7 \\times 7). We will also adjust strides carefully.\n",
        "\n",
        "### Updated Network Design (Receptive Field: ~44–50)\n",
        "\n",
        "```python\n",
        "\n",
        "```\n",
        "\n",
        "### Key Changes:\n",
        "\n",
        "1. **Dilation = 2**: I reduced the dilation to 2 to control the growth of the receptive field. Dilation of 2 still provides a nice expansion without overshooting the target receptive field.\n",
        "2. **Smaller Kernels ((5 \\times 5))**: The kernel sizes are reduced to (5 \\times 5) to control how much the receptive field expands at each layer. The larger the kernel, the larger the receptive field, so we keep it smaller to prevent overshooting.\n",
        "3. **Stride = 2**: I kept stride as 2 for downsampling after the first couple of convolution blocks. This keeps the feature map shrinking at a moderate rate.\n",
        "4. **Padding**: Adjusted padding to ensure that the convolution doesn't shrink the feature map dimensions too quickly. Since we are using dilation, we need to match the padding accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "### Receptive Field Calculation:\n",
        "\n",
        "Let’s now calculate the receptive field layer-by-layer for this architecture:\n",
        "\n",
        "#### **Conv Block 1** (with (5 X 5) kernels, dilation=2):\n",
        "\n",
        "* **First layer**: (5 s 5), dilation=2 → RF = (5 + (5 - 1) X 2 = 13)\n",
        "* **Second layer**: (5 x 5), dilation=2, stride=2 → RF = (13 + (5 - 1) X 2 = 21)\n",
        "\n",
        "#### **Conv Block 2** (with (5 X 5) kernels, dilation=2):\n",
        "\n",
        "* **First layer**: (5 X 5), dilation=2 → RF = (21 + (5 - 1) X 2 = 29)\n",
        "* **Second layer**: (5 X 5), dilation=2, stride=2 → RF = (29 + (5 - 1) X 2 = 37)\n",
        "\n",
        "#### **Conv Block 3** (with (5 X 5) kernels, dilation=2):\n",
        "\n",
        "* **First layer**: (5 X 5), dilation=2 → RF = (37 + (5 - 1) \\times 2 = 45)\n",
        "\n",
        "#### **Conv Block 4** (Depthwise convolution with dilation=2):\n",
        "\n",
        "* **Depthwise convolution layer**: (3 X 3), dilation=2 → RF = (45 + (3 - 1) \\times 2 = 49)\n",
        "* **Pointwise convolution**: (1 X 1) kernel → RF = **49** (no further change).\n",
        "\n",
        "---\n",
        "\n",
        "### Final Receptive Field:\n",
        "\n",
        "The receptive field is **49x49**, which falls within your desired range of **44 to 50**.\n",
        "\n",
        "### Summary of Adjustments:\n",
        "\n",
        "* **Receptive Field**: Now close to **49x49**, which is just under the 50 mark.\n",
        "* **Parameters**: The number of parameters has remained relatively constant because we used (5 \\times 5) kernels and dilation rather than larger kernels or more complex operations.\n",
        "* **Stride**: The stride was kept at 2 in some layers to avoid excessive downsampling, maintaining feature map dimensions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4FTb03p0fFLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "dropout_value = 0.0001\n",
        "\n",
        "class ciferNet(nn.Module):\n",
        "    def __init__(self, dilation=2):\n",
        "        super(ciferNet, self).__init__()\n",
        "\n",
        "        # Conv Block 1\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(5, 5), padding=2, dilation=dilation, bias=False),  # Kernel size: 5x5\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=1, padding=2, dilation=dilation, bias=False),  # Kernel size: 5x5, Changed stride to 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            # nn.Dropout(dropout_value),\n",
        "        )\n",
        "\n",
        "        # Conv Block 2\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=2, dilation=dilation, bias=False),  # Kernel size: 5x5\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=1, padding=2, dilation=dilation, bias=False),  # Kernel size: 5x5, Changed stride to 1\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            # nn.Dropout(dropout_value),\n",
        "        )\n",
        "\n",
        "        # Conv Block 3\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=4, dilation=dilation, bias=False),  # Kernel size: 5x5, Adjusted padding\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            # nn.Dropout(dropout_value),\n",
        "        )\n",
        "\n",
        "        # Conv Block 4 → Depthwise separable convolution with dilation\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=2, dilation=dilation, groups=128, bias=False),  # Depthwise conv with dilation\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 1), bias=False),  # Pointwise conv\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            # nn.Dropout(dropout_value),\n",
        "        )\n",
        "\n",
        "        # GAP and classifier\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Conv2d(in_channels=256, out_channels=10, kernel_size=(1, 1), bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "Sz69nwr8tH1E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = ciferNet().to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl2VtBeALu2W",
        "outputId": "d7a3f28e-7920-4115-a9c5-17dd7047c9ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]           1,200\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "       BatchNorm2d-3           [-1, 16, 28, 28]              32\n",
            "            Conv2d-4           [-1, 32, 28, 28]           4,608\n",
            "              ReLU-5           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-6           [-1, 32, 28, 28]              64\n",
            "            Conv2d-7           [-1, 64, 28, 28]          18,432\n",
            "              ReLU-8           [-1, 64, 28, 28]               0\n",
            "       BatchNorm2d-9           [-1, 64, 28, 28]             128\n",
            "           Conv2d-10           [-1, 64, 28, 28]          36,864\n",
            "             ReLU-11           [-1, 64, 28, 28]               0\n",
            "      BatchNorm2d-12           [-1, 64, 28, 28]             128\n",
            "           Conv2d-13          [-1, 128, 32, 32]          73,728\n",
            "             ReLU-14          [-1, 128, 32, 32]               0\n",
            "      BatchNorm2d-15          [-1, 128, 32, 32]             256\n",
            "           Conv2d-16          [-1, 128, 32, 32]           1,152\n",
            "             ReLU-17          [-1, 128, 32, 32]               0\n",
            "      BatchNorm2d-18          [-1, 128, 32, 32]             256\n",
            "           Conv2d-19          [-1, 256, 32, 32]          32,768\n",
            "             ReLU-20          [-1, 256, 32, 32]               0\n",
            "      BatchNorm2d-21          [-1, 256, 32, 32]             512\n",
            "AdaptiveAvgPool2d-22            [-1, 256, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 170,128\n",
            "Trainable params: 170,128\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 15.16\n",
            "Params size (MB): 0.65\n",
            "Estimated Total Size (MB): 15.82\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Train Phase\n",
        "train_transforms = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.0625,\n",
        "        scale_limit=0.1,\n",
        "        rotate_limit=45,\n",
        "        p=0.5\n",
        "    ),\n",
        "    A.CoarseDropout(\n",
        "        max_holes=1,\n",
        "        max_height=16,\n",
        "        max_width=16,\n",
        "        min_holes=1,\n",
        "        min_height=16,\n",
        "        min_width=16,\n",
        "        fill_value=(123.68, 116.78, 103.94),\n",
        "        mask_fill_value=None,\n",
        "        p=0.5\n",
        "    ),\n",
        "    A.Normalize(mean=(0.4741, 0.4727, 0.4733),\n",
        "                std=(0.2521, 0.2520, 0.2506)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Test Phase\n",
        "test_transforms = A.Compose([\n",
        "    A.Normalize(mean=(0.4741, 0.4727, 0.4733),\n",
        "                std=(0.2521, 0.2520, 0.2506)),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "Xo-t0YLCMA9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320798a4-2a67-492a-95ca-7f8791ae3375"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "/tmp/ipython-input-306781532.py:13: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value, mask_fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f9bc9ae"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CIFAR10Albumentations(Dataset):\n",
        "    def __init__(self, root, train=True, download=False, transform=None):\n",
        "        self.cifar10 = datasets.CIFAR10(root=root, train=train, download=download)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.cifar10[index]\n",
        "        if self.transform:\n",
        "            # Albumentations requires the image to be passed as a keyword argument 'image'\n",
        "            image = self.transform(image=np.array(image))['image'] # Convert PIL to numpy array before applying transform\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar10)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train = CIFAR10Albumentations('./data', train=True, download=True, transform=train_transforms)\n",
        "test = CIFAR10Albumentations('./data', train=False, download=True, transform=test_transforms)"
      ],
      "metadata": {
        "id": "YhgskLdmOZFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65a4583-926a-4660-c02d-0233fb91b3ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "train_dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "test_dataloader_args = dict(shuffle=False, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=False, batch_size=64)\n",
        "\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **train_dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **test_dataloader_args)"
      ],
      "metadata": {
        "id": "YqXdGOhTOyrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97958b0b-4db9-4a65-a2b0-4da889bea577"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss.item()) # Added .item()\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.float().argmax(dim=1, keepdim=True)  # get the index of the max log-probability, convert to float\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.float().argmax(dim=1, keepdim=True)  # get the index of the max log-probability, convert to float\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss) # Added .item()\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "metadata": {
        "id": "LT_W_lCuPb_b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  ciferNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 35\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    # scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "iEB6WDqsPdHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c868ac-91a0-4050-81ec-437bd66fbfc7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.9639126062393188 Batch_id=390 Accuracy=35.85: 100%|██████████| 391/391 [00:27<00:00, 14.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9218, Accuracy: 4575/10000 (45.75%)\n",
            "\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.5883114337921143 Batch_id=390 Accuracy=46.60: 100%|██████████| 391/391 [00:26<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4829, Accuracy: 5295/10000 (52.95%)\n",
            "\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2929236888885498 Batch_id=390 Accuracy=52.14: 100%|██████████| 391/391 [00:25<00:00, 15.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2819, Accuracy: 5785/10000 (57.85%)\n",
            "\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2105544805526733 Batch_id=390 Accuracy=55.43: 100%|██████████| 391/391 [00:25<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1841, Accuracy: 6041/10000 (60.41%)\n",
            "\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.2328954935073853 Batch_id=390 Accuracy=58.66: 100%|██████████| 391/391 [00:25<00:00, 15.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0820, Accuracy: 6341/10000 (63.41%)\n",
            "\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.1676756143569946 Batch_id=390 Accuracy=60.45: 100%|██████████| 391/391 [00:25<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.0123, Accuracy: 6620/10000 (66.20%)\n",
            "\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.0448118448257446 Batch_id=390 Accuracy=62.01: 100%|██████████| 391/391 [00:25<00:00, 15.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9498, Accuracy: 6781/10000 (67.81%)\n",
            "\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9256370663642883 Batch_id=390 Accuracy=63.95: 100%|██████████| 391/391 [00:25<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.9110, Accuracy: 6932/10000 (69.32%)\n",
            "\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.0602881908416748 Batch_id=390 Accuracy=64.89: 100%|██████████| 391/391 [00:25<00:00, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8934, Accuracy: 6954/10000 (69.54%)\n",
            "\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9776939153671265 Batch_id=390 Accuracy=66.33: 100%|██████████| 391/391 [00:25<00:00, 15.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8964, Accuracy: 6964/10000 (69.64%)\n",
            "\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9942640066146851 Batch_id=390 Accuracy=67.28: 100%|██████████| 391/391 [00:25<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8449, Accuracy: 7119/10000 (71.19%)\n",
            "\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6979392766952515 Batch_id=390 Accuracy=68.45: 100%|██████████| 391/391 [00:26<00:00, 15.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8338, Accuracy: 7160/10000 (71.60%)\n",
            "\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9518436193466187 Batch_id=390 Accuracy=68.99: 100%|██████████| 391/391 [00:25<00:00, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8132, Accuracy: 7251/10000 (72.51%)\n",
            "\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8628616333007812 Batch_id=390 Accuracy=69.46: 100%|██████████| 391/391 [00:25<00:00, 15.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7696, Accuracy: 7329/10000 (73.29%)\n",
            "\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7675178647041321 Batch_id=390 Accuracy=70.41: 100%|██████████| 391/391 [00:25<00:00, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7652, Accuracy: 7405/10000 (74.05%)\n",
            "\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7945773601531982 Batch_id=390 Accuracy=71.27: 100%|██████████| 391/391 [00:26<00:00, 14.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7360, Accuracy: 7510/10000 (75.10%)\n",
            "\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8120716214179993 Batch_id=390 Accuracy=71.44: 100%|██████████| 391/391 [00:25<00:00, 15.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7630, Accuracy: 7410/10000 (74.10%)\n",
            "\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8585365414619446 Batch_id=390 Accuracy=71.83: 100%|██████████| 391/391 [00:25<00:00, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7419, Accuracy: 7502/10000 (75.02%)\n",
            "\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8596149682998657 Batch_id=390 Accuracy=72.42: 100%|██████████| 391/391 [00:26<00:00, 14.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7016, Accuracy: 7596/10000 (75.96%)\n",
            "\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7158740758895874 Batch_id=390 Accuracy=72.72: 100%|██████████| 391/391 [00:25<00:00, 15.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6921, Accuracy: 7637/10000 (76.37%)\n",
            "\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.736467182636261 Batch_id=390 Accuracy=73.36: 100%|██████████| 391/391 [00:25<00:00, 15.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7012, Accuracy: 7658/10000 (76.58%)\n",
            "\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.9603772163391113 Batch_id=390 Accuracy=73.64: 100%|██████████| 391/391 [00:25<00:00, 15.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6861, Accuracy: 7674/10000 (76.74%)\n",
            "\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6653229594230652 Batch_id=390 Accuracy=74.06: 100%|██████████| 391/391 [00:26<00:00, 14.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6747, Accuracy: 7734/10000 (77.34%)\n",
            "\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7370064854621887 Batch_id=390 Accuracy=74.20: 100%|██████████| 391/391 [00:25<00:00, 15.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6797, Accuracy: 7712/10000 (77.12%)\n",
            "\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6129605770111084 Batch_id=390 Accuracy=74.79: 100%|██████████| 391/391 [00:26<00:00, 14.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6591, Accuracy: 7791/10000 (77.91%)\n",
            "\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8479812741279602 Batch_id=390 Accuracy=74.89: 100%|██████████| 391/391 [00:26<00:00, 14.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6692, Accuracy: 7723/10000 (77.23%)\n",
            "\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=1.041616439819336 Batch_id=390 Accuracy=75.41: 100%|██████████| 391/391 [00:26<00:00, 14.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6679, Accuracy: 7742/10000 (77.42%)\n",
            "\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7320215106010437 Batch_id=390 Accuracy=75.90: 100%|██████████| 391/391 [00:26<00:00, 14.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6464, Accuracy: 7823/10000 (78.23%)\n",
            "\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6113597750663757 Batch_id=390 Accuracy=75.92: 100%|██████████| 391/391 [00:26<00:00, 14.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6432, Accuracy: 7832/10000 (78.32%)\n",
            "\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.6311665177345276 Batch_id=390 Accuracy=75.83: 100%|██████████| 391/391 [00:26<00:00, 14.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6373, Accuracy: 7832/10000 (78.32%)\n",
            "\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.7378862500190735 Batch_id=390 Accuracy=76.44: 100%|██████████| 391/391 [00:26<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6230, Accuracy: 7874/10000 (78.74%)\n",
            "\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.839959442615509 Batch_id=390 Accuracy=76.87: 100%|██████████| 391/391 [00:26<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6337, Accuracy: 7848/10000 (78.48%)\n",
            "\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8395328521728516 Batch_id=390 Accuracy=76.66: 100%|██████████| 391/391 [00:26<00:00, 14.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6369, Accuracy: 7866/10000 (78.66%)\n",
            "\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.5861379504203796 Batch_id=390 Accuracy=77.10: 100%|██████████| 391/391 [00:26<00:00, 14.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6231, Accuracy: 7919/10000 (79.19%)\n",
            "\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.8220704793930054 Batch_id=390 Accuracy=77.06: 100%|██████████| 391/391 [00:26<00:00, 14.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.6185, Accuracy: 7912/10000 (79.12%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbK60O1UPfvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}